import contextlib
import math

# Fun√ß√£o para dividir √°udio WAV em partes de at√© 1400 segundos
def split_wav(audio_path, max_seconds=1400, overlap_seconds=15):
    import wave
    import math
    parts = []
    with contextlib.closing(wave.open(str(audio_path), 'rb')) as wf:
        framerate = wf.getframerate()
        nframes = wf.getnframes()
        sampwidth = wf.getsampwidth()
        nchannels = wf.getnchannels()
        duration = nframes / float(framerate)
        # Novo c√°lculo de partes considerando sobreposi√ß√£o
        step_frames = int((max_seconds - overlap_seconds) * framerate)
        part_frames = int(max_seconds * framerate)
        start_frame = 0
        idx = 1
        while start_frame < nframes:
            end_frame = min(start_frame + part_frames, nframes)
            wf.setpos(start_frame)
            frames = wf.readframes(end_frame - start_frame)
            part_path = audio_path.parent / f"{audio_path.stem}_part{idx}.wav"
            with wave.open(str(part_path), 'wb') as out:
                out.setnchannels(nchannels)
                out.setsampwidth(sampwidth)
                out.setframerate(framerate)
                out.writeframes(frames)
            print(f"[DEBUG] Parte {idx}: frames {start_frame} at√© {end_frame} (total: {end_frame-start_frame}) [sobreposi√ß√£o: {overlap_seconds}s]")
            parts.append(part_path)
            start_frame += step_frames
            idx += 1
    return parts
import os
import sys
import dotenv
from openai import OpenAI
import argparse
from pathlib import Path

# Requisitos: pip install vosk moviepy
# Baixe o modelo de voz para portugu√™s em: https://alphacephei.com/vosk/models
# Extraia o modelo em uma pasta chamada 'model' na raiz do projeto

import subprocess
from vosk import Model, KaldiRecognizer
import wave
def transcribe_audio_openai(audio_path):
    import os
    from openai import OpenAI
    import wave
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("[ERRO] OPENAI_API_KEY n√£o configurada no ambiente.")
        return None
    client = OpenAI(api_key=api_key)
    
    # Verificar dura√ß√£o do √°udio e calcular custo
    with wave.open(str(audio_path), "rb") as wf:
        duration = wf.getnframes() / float(wf.getframerate())
    
    # Calculadora de custo (baseado em $0.006 por minuto do GPT-4o-mini)
    duration_minutes = duration / 60
    estimated_cost = duration_minutes * 0.006  # $0.006 por minuto
    
    print(f"\nüí∞ CALCULADORA DE CUSTO OPENAI")
    print(f"üìä Dura√ß√£o do √°udio: {duration:.1f} segundos ({duration_minutes:.1f} minutos)")
    print(f"üíµ Custo estimado: ${estimated_cost:.4f} USD (~R$ {estimated_cost * 5.5:.2f})")
    
    if duration > 1400:
        parts_needed = math.ceil(duration / 1400)
        print(f"‚ö†Ô∏è  √Åudio ser√° dividido em {parts_needed} partes (limite: 1400s por parte)")
    
    confirm = input(f"\nüîÑ Deseja prosseguir com a transcri√ß√£o? [s/n]: ").strip().lower()
    if confirm != 's':
        print("‚ùå Transcri√ß√£o cancelada pelo usu√°rio.")
        return None
    
    # Par√¢metros de chunking e prompt customizado
    max_seconds = int(os.getenv("OPENAI_MAX_SECONDS", "900"))
    overlap_seconds = int(os.getenv("OPENAI_OVERLAP_SECONDS", "15"))
    custom_prompt = os.getenv("OPENAI_TRANSCRIBE_PROMPT", "Transcreva todo o conte√∫do do √°udio, sem resumir ou omitir partes. Mantenha a ordem e o m√°ximo de detalhes poss√≠vel.")
    print(f"[INFO] Usando {max_seconds}s por parte, sobreposi√ß√£o de {overlap_seconds}s e prompt customizado para OpenAI.")
    if duration > max_seconds:
        print(f"[INFO] √Åudio com {duration:.2f}s excede limite do OpenAI. Dividindo em partes...")
        parts = split_wav(audio_path, max_seconds=max_seconds, overlap_seconds=overlap_seconds)
        full_transcript = ""
        print(f"[DEBUG] Total de partes criadas: {len(parts)}")
        parte_vazia = 0
        for idx, part in enumerate(parts, 1):
            print(f"[INFO] Transcrevendo parte {idx}/{len(parts)}: {part.name} ({part.stat().st_size} bytes)")
            tentativas = 0
            part_text = ""
            while tentativas < 2 and not part_text:
                tentativas += 1
                try:
                    with open(part, "rb") as audio_file:
                        transcription = client.audio.transcriptions.create(
                            model="gpt-4o-mini-transcribe",
                            file=audio_file,
                            prompt=custom_prompt
                        )
                        part_text = transcription.text.strip()
                        print(f"[DEBUG] Parte {idx} - Tentativa {tentativas} - Caracteres transcritos: {len(part_text)}")
                        if part_text:
                            print(f"[DEBUG] Parte {idx} - Primeiros 100 chars: {part_text[:100]}...")
                            print(f"[DEBUG] Parte {idx} - √öltimos 100 chars: ...{part_text[-100:]}")
                        else:
                            print(f"[AVISO] Parte {idx} tentativa {tentativas} resultou em transcri√ß√£o vazia!")
                except Exception as e:
                    print(f"[ERRO] Falha na transcri√ß√£o da parte {idx} tentativa {tentativas}: {e}")
                    break
            if part_text:
                full_transcript += part_text + "\n\n"
            else:
                parte_vazia += 1
                print(f"[ERRO] Parte {idx} ({part.name}) ignorada por estar vazia ap√≥s {tentativas} tentativas.")
        # Limpar arquivos tempor√°rios das partes
        for part in parts:
            try:
                os.remove(part)
            except Exception:
                pass
        print(f"[DEBUG] Transcri√ß√£o final - Total de caracteres: {len(full_transcript)}")
        print(f"[INFO] Partes ignoradas por estarem vazias: {parte_vazia} de {len(parts)}")
        if parte_vazia:
            print(f"[ALERTA] Algumas partes do √°udio n√£o foram transcritas. Considere diminuir o tamanho m√°ximo por parte ou revisar o √°udio original.")
        print("[INFO] Transcri√ß√£o conclu√≠da via OpenAI (partes).")
        return full_transcript.strip()
    else:
        with open(audio_path, "rb") as audio_file:
            try:
                transcription = client.audio.transcriptions.create(
                    model="gpt-4o-mini-transcribe",
                    file=audio_file,
                    prompt=custom_prompt
                )
                print("[INFO] Transcri√ß√£o conclu√≠da via OpenAI.")
                return transcription.text
            except Exception as e:
                print(f"[ERRO] Falha na transcri√ß√£o via OpenAI: {e}")
                return None

BASE_DIR = Path(__file__).parent.resolve()
INPUT_DIR = BASE_DIR / "input"
OUTPUT_DIR = BASE_DIR / "output"
MODEL_DIR = BASE_DIR / "model"

def extract_audio(video_path, audio_path):
    # Extrai √°udio usando ffmpeg (precisa estar instalado no sistema)
    cmd = [
        "ffmpeg",
        "-y",  # overwrite output
        "-i", str(video_path),
        "-vn",  # no video
        "-acodec", "pcm_s16le",
        "-ar", "16000",
        "-ac", "1",
        str(audio_path)
    ]
    print(f"[INFO] Extraindo √°udio do v√≠deo para arquivo WAV...")
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        # Mensagens do ffmpeg ocultas
    except subprocess.CalledProcessError as e:
        print(f"[ERRO] Falha ao extrair √°udio do v√≠deo: {e}")
    if os.path.exists(audio_path):
        print(f"[INFO] √Åudio extra√≠do com sucesso.")
    else:
        print(f"[ERRO] Arquivo de √°udio n√£o foi criado.")

def transcribe_audio(audio_path):
    import json
    log_path = OUTPUT_DIR / "transcricao-debug.log"
    if not MODEL_DIR.exists():
        msg = "Modelo Vosk n√£o encontrado. Baixe e extraia em ./model"
        print(msg)
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(msg + "\n")
        return ""
    print(f"[INFO] Iniciando transcri√ß√£o do √°udio...")
    try:
        wf = wave.open(str(audio_path), "rb")
        info = f"WAV channels: {wf.getnchannels()} | Sample width: {wf.getsampwidth()} | Frame rate: {wf.getframerate()} | Frames: {wf.getnframes()}"
        duration = wf.getnframes() / float(wf.getframerate())
        print(f"[INFO] Dura√ß√£o do √°udio: {duration:.2f} segundos")
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(f"Arquivo: {audio_path}\n{info}\nDura√ß√£o: {duration:.2f} segundos\n")
    except Exception as e:
        msg = f"Erro ao abrir arquivo WAV: {e}"
        print(f"[ERRO] Erro ao abrir arquivo WAV: {e}")
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(msg + "\n")
        return ""
    try:
        model = Model(str(MODEL_DIR))
    except Exception as e:
        msg = f"Erro ao carregar modelo Vosk: {e}"
        print(f"[ERRO] Erro ao carregar modelo Vosk: {e}")
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(msg + "\n")
        return ""
    try:
        rec = KaldiRecognizer(model, wf.getframerate())
        rec.SetWords(True)
        results = []
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            if rec.AcceptWaveform(data):
                res = rec.Result()
                results.append(res)
        results.append(rec.FinalResult())
        transcript = "\n".join([json.loads(r).get("text","") for r in results])
        print(f"[INFO] Transcri√ß√£o conclu√≠da.")
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(f"Transcri√ß√£o: {transcript[:200]}...\n")
        return transcript
    except Exception as e:
        msg = f"Erro ao transcrever √°udio: {e}"
        print(f"[ERRO] Erro ao transcrever √°udio: {e}")
        with open(log_path, "a", encoding="utf-8") as log:
            log.write(msg + "\n")
        return ""

def save_markdown(transcription, client_name, video_file):
    output_file = OUTPUT_DIR / f"transcricao-de-reuniao-{client_name}.md"
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# Transcri√ß√£o de Reuni√£o - {client_name}\n\n")
        f.write(f"**Arquivo de origem:** {video_file}\n\n")
        f.write(transcription)
    print(f"Transcri√ß√£o salva em: {output_file}")

def main():
    print("\n=== Transcri√ß√£o de v√≠deos ===\n")
    # Carregar vari√°veis do .env se existir
    dotenv.load_dotenv()

    print("Escolha o motor de transcri√ß√£o:")
    print("[1] Vosk (offline)")
    print("[2] OpenAI GPT-4o (requer token)")
    metodo = input("Digite 1 ou 2: ").strip()
    usar_openai = metodo == "2"

    videos = list(INPUT_DIR.glob("*.mp4"))
    if not videos:
        print("Nenhum v√≠deo mp4 encontrado na pasta input/")
        sys.exit(1)

    print("Selecione os v√≠deos que deseja transcrever:")
    for idx, video in enumerate(videos, 1):
        print(f"[{idx}] {video.name}")
    escolha = input("Digite os n√∫meros dos v√≠deos separados por v√≠rgula (ex: 1,3): ").strip()
    indices = [int(i)-1 for i in escolha.split(",") if i.strip().isdigit() and 0 < int(i) <= len(videos)]
    selecionados = [videos[i] for i in indices]
    if not selecionados:
        print("Nenhum v√≠deo selecionado. Encerrando.")
        sys.exit(1)

    cliente = input("Digite o nome do cliente para nomear o arquivo de sa√≠da: ").strip()
    if not cliente:
        print("Nome do cliente n√£o informado. Encerrando.")
        sys.exit(1)

    import shutil
    temp_dir = BASE_DIR / "temp"
    temp_dir.mkdir(exist_ok=True)
    temp_files = []
    for video in selecionados:
        print(f"[INFO] Processando v√≠deo: {video.name}")
        audio_path = temp_dir / f"{video.stem}.wav"
        extract_audio(video, audio_path)
        temp_files.append(audio_path)
        if os.path.exists(audio_path):
            if usar_openai:
                transcription = transcribe_audio_openai(audio_path)
                # Se transcription for None, pode ser erro t√©cnico ou cancelamento do usu√°rio
                if transcription is None:
                    # Verifica se foi erro t√©cnico (n√£o cancelamento)
                    # O transcribe_audio_openai j√° mostra mensagem de cancelamento
                    # Aqui s√≥ oferece Vosk se n√£o foi cancelamento
                    # Se o usu√°rio cancelou, n√£o faz nada, s√≥ pula para o pr√≥ximo v√≠deo
                    continue
            else:
                transcription = transcribe_audio(audio_path)
            save_markdown(transcription, cliente, video.name)
            print(f"[INFO] Arquivo de √°udio mantido em: {audio_path}")
        else:
            print(f"[ERRO] N√£o foi poss√≠vel extrair o √°udio. Pulando transcri√ß√£o.")

    # Limpeza autom√°tica da pasta temp (sempre executada)
    try:
        shutil.rmtree(temp_dir)
        print("[INFO] Pasta temp/ removida com sucesso.")
    except Exception as e:
        print(f"[ERRO] Falha ao remover pasta temp/: {e}")

if __name__ == "__main__":
    main()
